{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_NXDvRLTLam"
   },
   "source": [
    "# Challenge Data QRT Stock Return Prediction 2020\n",
    "\n",
    "This notebook illustrates my attempt on the 2020 Data Challenge that tries to predict the return of a stock in the US market using historical data over a recent period of 20 days.\n",
    "\n",
    "## Used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OL9ZSBSpTLK1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV,KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBTmhhj9XX8b"
   },
   "source": [
    "## Loading data\n",
    "\n",
    "The train and test inputs are composed of 46 features.\n",
    "\n",
    "The target of this challenge is to predict the column `RET` that corresponds to the fact that the **return is in the top 50% of highest stock returns** : if it is the case, RET = True, else False. This problem can be represented by a classification problem where the first class will be 1 if RET = True and the second class 0 if RET = False.\n",
    "\n",
    "Since the median is very close to 0, this information should not change much with the idea to predict the sign of the return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "6PZSmwhJZPtF",
    "outputId": "6374a633-40cf-47c1-fb30-887554751b14"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>STOCK</th>\n",
       "      <th>INDUSTRY</th>\n",
       "      <th>INDUSTRY_GROUP</th>\n",
       "      <th>SECTOR</th>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <th>RET_1</th>\n",
       "      <th>VOLUME_1</th>\n",
       "      <th>RET_2</th>\n",
       "      <th>VOLUME_2</th>\n",
       "      <th>...</th>\n",
       "      <th>VOLUME_16</th>\n",
       "      <th>RET_17</th>\n",
       "      <th>VOLUME_17</th>\n",
       "      <th>RET_18</th>\n",
       "      <th>VOLUME_18</th>\n",
       "      <th>RET_19</th>\n",
       "      <th>VOLUME_19</th>\n",
       "      <th>RET_20</th>\n",
       "      <th>VOLUME_20</th>\n",
       "      <th>RET</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>-0.015748</td>\n",
       "      <td>0.147931</td>\n",
       "      <td>-0.015504</td>\n",
       "      <td>0.179183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630899</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>-0.379412</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>-0.110597</td>\n",
       "      <td>-0.012959</td>\n",
       "      <td>0.174521</td>\n",
       "      <td>-0.002155</td>\n",
       "      <td>-0.000937</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>104</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.090580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.018518</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.028777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.034722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>142</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.096282</td>\n",
       "      <td>-0.058896</td>\n",
       "      <td>0.084771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010336</td>\n",
       "      <td>-0.017612</td>\n",
       "      <td>-0.354333</td>\n",
       "      <td>-0.006562</td>\n",
       "      <td>-0.519391</td>\n",
       "      <td>-0.012101</td>\n",
       "      <td>-0.356157</td>\n",
       "      <td>-0.006867</td>\n",
       "      <td>-0.308868</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031298</td>\n",
       "      <td>-0.429540</td>\n",
       "      <td>0.007756</td>\n",
       "      <td>-0.089919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012105</td>\n",
       "      <td>0.033824</td>\n",
       "      <td>-0.290178</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>-0.663834</td>\n",
       "      <td>-0.013520</td>\n",
       "      <td>-0.562126</td>\n",
       "      <td>-0.036745</td>\n",
       "      <td>-0.631458</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>-0.847155</td>\n",
       "      <td>-0.039302</td>\n",
       "      <td>-0.943033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277083</td>\n",
       "      <td>-0.012659</td>\n",
       "      <td>0.139086</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>-0.017547</td>\n",
       "      <td>0.004256</td>\n",
       "      <td>0.579510</td>\n",
       "      <td>-0.040817</td>\n",
       "      <td>0.802806</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418590</th>\n",
       "      <td>223</td>\n",
       "      <td>5703</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>77</td>\n",
       "      <td>0.021843</td>\n",
       "      <td>-0.217823</td>\n",
       "      <td>-0.021703</td>\n",
       "      <td>-0.125333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161543</td>\n",
       "      <td>0.007785</td>\n",
       "      <td>-0.141487</td>\n",
       "      <td>0.008205</td>\n",
       "      <td>-0.268215</td>\n",
       "      <td>-0.058448</td>\n",
       "      <td>0.057577</td>\n",
       "      <td>0.031174</td>\n",
       "      <td>-0.163350</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418591</th>\n",
       "      <td>223</td>\n",
       "      <td>5705</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>91</td>\n",
       "      <td>-0.006920</td>\n",
       "      <td>-0.375251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.029437</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.955492</td>\n",
       "      <td>-0.016221</td>\n",
       "      <td>-0.171172</td>\n",
       "      <td>0.007819</td>\n",
       "      <td>1.540184</td>\n",
       "      <td>-0.015396</td>\n",
       "      <td>-0.502337</td>\n",
       "      <td>-0.011073</td>\n",
       "      <td>0.632885</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418592</th>\n",
       "      <td>223</td>\n",
       "      <td>5709</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.021869</td>\n",
       "      <td>-0.978856</td>\n",
       "      <td>-0.005929</td>\n",
       "      <td>-1.026267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.476550</td>\n",
       "      <td>0.029714</td>\n",
       "      <td>-0.385972</td>\n",
       "      <td>-0.069148</td>\n",
       "      <td>1.780169</td>\n",
       "      <td>-0.082927</td>\n",
       "      <td>1.581453</td>\n",
       "      <td>0.098607</td>\n",
       "      <td>6.956960</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418593</th>\n",
       "      <td>223</td>\n",
       "      <td>5710</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>0.012248</td>\n",
       "      <td>-0.627169</td>\n",
       "      <td>0.010925</td>\n",
       "      <td>-0.842108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.210079</td>\n",
       "      <td>0.023729</td>\n",
       "      <td>0.813948</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>-0.868576</td>\n",
       "      <td>-0.051155</td>\n",
       "      <td>0.371052</td>\n",
       "      <td>0.041238</td>\n",
       "      <td>0.045695</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418594</th>\n",
       "      <td>223</td>\n",
       "      <td>5713</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>0.076162</td>\n",
       "      <td>-1.325986</td>\n",
       "      <td>-0.000988</td>\n",
       "      <td>0.198856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277896</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>0.056942</td>\n",
       "      <td>-0.001896</td>\n",
       "      <td>-0.521932</td>\n",
       "      <td>-0.008366</td>\n",
       "      <td>-0.377668</td>\n",
       "      <td>0.003679</td>\n",
       "      <td>-1.393662</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418595 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE  STOCK  INDUSTRY  INDUSTRY_GROUP  SECTOR  SUB_INDUSTRY     RET_1  \\\n",
       "ID                                                                              \n",
       "0          0      2        18               5       3            44 -0.015748   \n",
       "1          0      3        43              15       6           104  0.003984   \n",
       "2          0      4        57              20       8           142  0.000440   \n",
       "3          0      8         1               1       1             2  0.031298   \n",
       "4          0     14        36              12       5            92  0.027273   \n",
       "...      ...    ...       ...             ...     ...           ...       ...   \n",
       "418590   223   5703        32              10       4            77  0.021843   \n",
       "418591   223   5705        35              12       5            91 -0.006920   \n",
       "418592   223   5709         2               1       1             5  0.021869   \n",
       "418593   223   5710        33              10       4            83  0.012248   \n",
       "418594   223   5713        26               7       4            60  0.076162   \n",
       "\n",
       "        VOLUME_1     RET_2  VOLUME_2  ...  VOLUME_16    RET_17  VOLUME_17  \\\n",
       "ID                                    ...                                   \n",
       "0       0.147931 -0.015504  0.179183  ...   0.630899  0.003254  -0.379412   \n",
       "1            NaN -0.090580       NaN  ...        NaN  0.003774        NaN   \n",
       "2      -0.096282 -0.058896  0.084771  ...  -0.010336 -0.017612  -0.354333   \n",
       "3      -0.429540  0.007756 -0.089919  ...   0.012105  0.033824  -0.290178   \n",
       "4      -0.847155 -0.039302 -0.943033  ...  -0.277083 -0.012659   0.139086   \n",
       "...          ...       ...       ...  ...        ...       ...        ...   \n",
       "418590 -0.217823 -0.021703 -0.125333  ...  -0.161543  0.007785  -0.141487   \n",
       "418591 -0.375251  0.000000 -0.029437  ...  -0.955492 -0.016221  -0.171172   \n",
       "418592 -0.978856 -0.005929 -1.026267  ...  -0.476550  0.029714  -0.385972   \n",
       "418593 -0.627169  0.010925 -0.842108  ...  -0.210079  0.023729   0.813948   \n",
       "418594 -1.325986 -0.000988  0.198856  ...   0.277896 -0.037037   0.056942   \n",
       "\n",
       "          RET_18  VOLUME_18    RET_19  VOLUME_19    RET_20  VOLUME_20    RET  \n",
       "ID                                                                            \n",
       "0       0.008752  -0.110597 -0.012959   0.174521 -0.002155  -0.000937   True  \n",
       "1      -0.018518        NaN -0.028777        NaN -0.034722        NaN   True  \n",
       "2      -0.006562  -0.519391 -0.012101  -0.356157 -0.006867  -0.308868  False  \n",
       "3      -0.001468  -0.663834 -0.013520  -0.562126 -0.036745  -0.631458  False  \n",
       "4       0.004237  -0.017547  0.004256   0.579510 -0.040817   0.802806  False  \n",
       "...          ...        ...       ...        ...       ...        ...    ...  \n",
       "418590  0.008205  -0.268215 -0.058448   0.057577  0.031174  -0.163350  False  \n",
       "418591  0.007819   1.540184 -0.015396  -0.502337 -0.011073   0.632885  False  \n",
       "418592 -0.069148   1.780169 -0.082927   1.581453  0.098607   6.956960   True  \n",
       "418593  0.026087  -0.868576 -0.051155   0.371052  0.041238   0.045695   True  \n",
       "418594 -0.001896  -0.521932 -0.008366  -0.377668  0.003679  -1.393662  False  \n",
       "\n",
       "[418595 rows x 47 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_csv = str(sys.path[0])+'\\\\x_train.csv'\n",
    "y_train_csv = str(sys.path[0])+'\\\\y_train.csv'\n",
    "\n",
    "x_train = pd.read_csv(x_train_csv, index_col='ID')\n",
    "y_train = pd.read_csv(y_train_csv, index_col='ID')\n",
    "train = pd.concat([x_train, y_train], axis=1)\n",
    "test = pd.read_csv('x_test.csv', index_col='ID')\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwvxgyZGcq81"
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "The main principal problem in this challenge would be to deal with the noise. To do that, we could create some feature that aggregate features with some statistics.\n",
    "\n",
    "The next cell computes statistics on a given target conditionally to some features. For example, we will try to generate a feature that describe the mean, min, max and standard deviation of `RET_1` conditionally to the `SECTOR` and the `DATE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6YV01eQXe3jt"
   },
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "new_features = []\n",
    "\n",
    "# Conditional aggregated features\n",
    "shifts = [1]  # We could choose some different shifts\n",
    "statistics = ['mean','min','max','std']  # The type of statistic\n",
    "gb_features = ['SECTOR', 'DATE']\n",
    "target_feature = 'RET'\n",
    "tmp_name = '_'.join(gb_features)\n",
    "# The shift represents the number of days before the day we consider.\n",
    "for shift in shifts:\n",
    "    for stat in statistics:\n",
    "        name = f'{target_feature}_{shift}_{tmp_name}_{stat}'\n",
    "        feat = f'{target_feature}_{shift}'\n",
    "        new_features.append(name)\n",
    "        for data in [train, test]:\n",
    "            data[name] = data.groupby(gb_features)[feat].transform(stat) # To apply the transformation stat\n",
    "            # on the conditional data (as it was grouped by the parameters that we want to condition with respect to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dz9x6R1zh4u3"
   },
   "source": [
    "## Feature selection\n",
    "\n",
    "To reduce the number of feature (and the noise) we are only going to consider the 5 last days of `RET` and `VOLUME` in addition to the newly created feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "id": "1J4p90j7iZn0",
    "outputId": "535b4eac-f710-46f8-d03f-295a1edd5e20"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RET_1</th>\n",
       "      <th>RET_2</th>\n",
       "      <th>RET_3</th>\n",
       "      <th>RET_4</th>\n",
       "      <th>RET_5</th>\n",
       "      <th>VOLUME_1</th>\n",
       "      <th>VOLUME_2</th>\n",
       "      <th>VOLUME_3</th>\n",
       "      <th>VOLUME_4</th>\n",
       "      <th>VOLUME_5</th>\n",
       "      <th>RET_1_SECTOR_DATE_mean</th>\n",
       "      <th>RET_1_SECTOR_DATE_min</th>\n",
       "      <th>RET_1_SECTOR_DATE_max</th>\n",
       "      <th>RET_1_SECTOR_DATE_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.015748</td>\n",
       "      <td>-0.015504</td>\n",
       "      <td>0.010972</td>\n",
       "      <td>-0.014672</td>\n",
       "      <td>0.016483</td>\n",
       "      <td>0.147931</td>\n",
       "      <td>0.179183</td>\n",
       "      <td>0.033832</td>\n",
       "      <td>-0.362868</td>\n",
       "      <td>-0.972920</td>\n",
       "      <td>0.009178</td>\n",
       "      <td>-0.105345</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.024243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003984</td>\n",
       "      <td>-0.090580</td>\n",
       "      <td>0.018826</td>\n",
       "      <td>-0.025540</td>\n",
       "      <td>-0.038062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006477</td>\n",
       "      <td>-0.119048</td>\n",
       "      <td>0.314424</td>\n",
       "      <td>0.034339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.058896</td>\n",
       "      <td>-0.009042</td>\n",
       "      <td>0.024852</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>-0.096282</td>\n",
       "      <td>0.084771</td>\n",
       "      <td>-0.298777</td>\n",
       "      <td>-0.157421</td>\n",
       "      <td>0.091455</td>\n",
       "      <td>0.013449</td>\n",
       "      <td>-0.159287</td>\n",
       "      <td>0.242685</td>\n",
       "      <td>0.038375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.031298</td>\n",
       "      <td>0.007756</td>\n",
       "      <td>-0.004632</td>\n",
       "      <td>-0.019677</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>-0.429540</td>\n",
       "      <td>-0.089919</td>\n",
       "      <td>-0.639737</td>\n",
       "      <td>-0.940163</td>\n",
       "      <td>-0.882464</td>\n",
       "      <td>0.017253</td>\n",
       "      <td>-0.034853</td>\n",
       "      <td>0.082619</td>\n",
       "      <td>0.022464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.027273</td>\n",
       "      <td>-0.039302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>-0.847155</td>\n",
       "      <td>-0.943033</td>\n",
       "      <td>-1.180629</td>\n",
       "      <td>-1.313896</td>\n",
       "      <td>-1.204398</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>-0.134857</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.022162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       RET_1     RET_2     RET_3     RET_4     RET_5  VOLUME_1  VOLUME_2  \\\n",
       "ID                                                                         \n",
       "0  -0.015748 -0.015504  0.010972 -0.014672  0.016483  0.147931  0.179183   \n",
       "1   0.003984 -0.090580  0.018826 -0.025540 -0.038062       NaN       NaN   \n",
       "2   0.000440 -0.058896 -0.009042  0.024852  0.009354 -0.096282  0.084771   \n",
       "3   0.031298  0.007756 -0.004632 -0.019677  0.003544 -0.429540 -0.089919   \n",
       "4   0.027273 -0.039302  0.000000  0.000000  0.022321 -0.847155 -0.943033   \n",
       "\n",
       "    VOLUME_3  VOLUME_4  VOLUME_5  RET_1_SECTOR_DATE_mean  \\\n",
       "ID                                                         \n",
       "0   0.033832 -0.362868 -0.972920                0.009178   \n",
       "1        NaN       NaN       NaN                0.006477   \n",
       "2  -0.298777 -0.157421  0.091455                0.013449   \n",
       "3  -0.639737 -0.940163 -0.882464                0.017253   \n",
       "4  -1.180629 -1.313896 -1.204398                0.006241   \n",
       "\n",
       "    RET_1_SECTOR_DATE_min  RET_1_SECTOR_DATE_max  RET_1_SECTOR_DATE_std  \n",
       "ID                                                                       \n",
       "0               -0.105345               0.134000               0.024243  \n",
       "1               -0.119048               0.314424               0.034339  \n",
       "2               -0.159287               0.242685               0.038375  \n",
       "3               -0.034853               0.082619               0.022464  \n",
       "4               -0.134857               0.074074               0.022162  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'RET' # Here we define the final metric that is considered for the classification, so the output.\n",
    "\n",
    "n_shifts = 5  # If we don't want all the shifts to reduce noise\n",
    "features = ['RET_%d' % (i + 1) for i in range(n_shifts)] # features = ['RET_1', 'RET_2', 'RET_3', 'RET_4', 'RET_5']\n",
    "features += ['VOLUME_%d' % (i + 1) for i in range(n_shifts)] # features = ['RET_1', 'RET_2', 'RET_3', 'RET_4', 'RET_5', 'VOLUME_1', 'VOLUME_2', 'VOLUME_3', 'VOLUME_4', 'VOLUME_5']\n",
    "features += new_features  # The conditional features created before are all stored in this list with the RET_ and VOLUME_ ones.\n",
    "train[features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYchrfEBlpcQ"
   },
   "source": [
    "## Model and local score\n",
    "\n",
    "The model we choose for this test is a Random Forest (RF) model. We will consider a large number of tree with a quiet small depth to reduce the possibility of overfitting. Those parameters can be changed in the future and optimized. The missing values will simply be filled with 0 for the moment. A KFold is done on the dates (using `DATE`) for a local scoring of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLgM97djobFp",
    "outputId": "cc1553c5-9338-4856-e8d5-83b2f69c988c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Accuracy: 51.95%\n",
      "Fold 2 - Accuracy: 50.38%\n",
      "Fold 3 - Accuracy: 51.09%\n",
      "Fold 4 - Accuracy: 51.70%\n",
      "Accuracy: 51.28% [50.68 ; 51.89] (+- 0.61)\n"
     ]
    }
   ],
   "source": [
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "\n",
    "# A quiet large number of trees with low depth is needed to prevent overfits\n",
    "# We now define the parameters of the Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 2**3,\n",
    "    'random_state': 0,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# We store the unique dates in the train and in the test data.\n",
    "train_dates = train['DATE'].unique()\n",
    "test_dates = test['DATE'].unique()\n",
    "\n",
    "n_splits = 4\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "# Let's define the splits with the Kfold, the data will be split into four parts, and each\n",
    "# will be used as the test set once while the other three are used for training.\n",
    "splits = KFold(n_splits=n_splits, random_state=0,\n",
    "               shuffle=True).split(train_dates)\n",
    "\n",
    "for i, (local_train_dates_ids, local_test_dates_ids) in enumerate(splits):\n",
    "    # Here we basically use the same principle as the train_test_split function but we do it n_splits times.\n",
    "    local_train_dates = train_dates[local_train_dates_ids]\n",
    "    local_test_dates = train_dates[local_test_dates_ids]\n",
    "\n",
    "    local_train_ids = train['DATE'].isin(local_train_dates)\n",
    "    local_test_ids = train['DATE'].isin(local_test_dates)\n",
    "\n",
    "    X_local_train = X_train.loc[local_train_ids]\n",
    "    y_local_train = y_train.loc[local_train_ids]\n",
    "    X_local_test = X_train.loc[local_test_ids]\n",
    "    y_local_test = y_train.loc[local_test_ids]\n",
    "\n",
    "    # We fill the NaN values with 0.\n",
    "    X_local_train = X_local_train.fillna(0)\n",
    "    X_local_test = X_local_test.fillna(0)\n",
    "\n",
    "    # Debugging NaN checks in case the file is not fully loaded for example (as working on Google Colab)\n",
    "    assert not X_local_train.isnull().values.any(), \"X_local_train contains NaN values\"\n",
    "    assert not X_local_test.isnull().values.any(), \"X_local_test contains NaN values\"\n",
    "    assert not y_local_train.isnull().values.any(), \"y_local_train contains NaN values\"\n",
    "    assert not y_local_test.isnull().values.any(), \"y_local_test contains NaN values\"\n",
    "\n",
    "    # Let's now fit the Random Forest model with the parameters defined above.\n",
    "    model = RandomForestClassifier(**rf_params)\n",
    "    model.fit(X_local_train, y_local_train)\n",
    "\n",
    "    # We apply the model trained on the X_local_train data to the X_local_test data\n",
    "    y_local_pred = model.predict_proba(X_local_test)[:, 1]\n",
    "\n",
    "    # We create a copy of the train dataframe\n",
    "    sub = train.loc[local_test_ids].copy()\n",
    "    # We add the output of the trained model on the X_local_test data\n",
    "    sub['pred'] = y_local_pred\n",
    "\n",
    "    # We want the prediction to be True if the value is superior to the value of the median as it would mean that\n",
    "    # the return is in the top 50% of highest stock returns.\n",
    "    y_local_pred = sub.groupby('DATE')['pred'].transform(lambda x: x > x.median()).values\n",
    "\n",
    "    models.append(model)\n",
    "    score = accuracy_score(y_local_test, y_local_pred)\n",
    "    scores.append(score)\n",
    "    # We display the accuracy for each fold\n",
    "    print(f\"Fold {i+1} - Accuracy: {score* 100:.2f}%\")\n",
    "\n",
    "# Finally, we will print the mean result of the n_splits folds, with its standard deviation defining a confidence interval\n",
    "mean = np.mean(scores)*100\n",
    "std = np.std(scores)*100\n",
    "u = (mean + std)\n",
    "l = (mean - std)\n",
    "print(f'Accuracy: {mean:.2f}% [{l:.2f} ; {u:.2f}] (+- {std:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "kdZD2uSA8t5o",
    "outputId": "07156544-04dd-4271-b989-94882b66c8e1"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iteritems'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-100b2e6345e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfeature_importances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_importances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'h'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_importances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Ethan\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m             )\n\u001b[0;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ethan\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mbarplot\u001b[1;34m(x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge, ax, **kwargs)\u001b[0m\n\u001b[0;32m   3177\u001b[0m ):\n\u001b[0;32m   3178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3179\u001b[1;33m     plotter = _BarPlotter(x, y, hue, data, order, hue_order,\n\u001b[0m\u001b[0;32m   3180\u001b[0m                           \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3181\u001b[0m                           \u001b[0morient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ethan\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[0;32m   1582\u001b[0m                  errwidth, capsize, dodge):\n\u001b[0;32m   1583\u001b[0m         \u001b[1;34m\"\"\"Initialize the plotter.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1584\u001b[1;33m         self.establish_variables(x, y, hue, data, orient,\n\u001b[0m\u001b[0;32m   1585\u001b[0m                                  order, hue_order, units)\n\u001b[0;32m   1586\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ethan\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mestablish_variables\u001b[1;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;31m# Convert to a list of arrays, the common representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0miter_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m                 \u001b[0mplot_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miter_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ethan\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5991\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iteritems'"
     ]
    }
   ],
   "source": [
    "# We can visualize which feature is the most important in the model we just built.\n",
    "# We can expect it to be the conditional features we created.\n",
    "feature_importances = pd.DataFrame([model.feature_importances_ for model in models], columns=features)\n",
    "\n",
    "sns.barplot(data=feature_importances, orient='h', order=feature_importances.mean().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FofZMFTE0jY7"
   },
   "source": [
    "# Feature Engineering Bis\n",
    "\n",
    "Now, we are going to try to improve the previous feature engineering made on the data. As ideas of improvement, we could try to change shifts by adding more values and calculate statistics on a given target, here Return, conditionaly to some new features. For the record, we already calculated multiple statistics. The next variation of the method could be to change the target variable as the primary focus was on the Return as it is the variable we would like to predict, but it can also be done on the volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HvFsnoZ74h5V"
   },
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('x_train.csv', index_col='ID')\n",
    "y_train = pd.read_csv('y_train.csv', index_col='ID')\n",
    "train = pd.concat([x_train, y_train], axis=1)\n",
    "test = pd.read_csv('x_test.csv', index_col='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VO0BDsGt8ZSs"
   },
   "source": [
    "We handle the missing values differently than previously where we just decided to fill them with NaN values :\n",
    "- For VOLUME: We replace missing values with the average volume of the same sector for the same date. If this condition can't be met, for example if no data is available for that particular date, we average the sector for all dates. Finally, if this fallback doesn't work, we can use the global mean of the columns, so for all stocks.\n",
    "- For RET: We replace missing values using the average of the values in the same row’s immediate left (RET_shift-1) and right (RET_shift+1) columns, so the day before and the day after. If this condition can't be met, so if the considered column is the first or the last of the RET_ columns of the dataframe, we fallback to the previous or the next column, depending on the one available. If one of them has also NaN values, we use the mean return for the same sector and date. And finally, we fallback to the global mean of the returns for the column.\n",
    "\n",
    "This could also be done by creating some fallback strategies based on the industry_group or the industry instead of the sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ygXla4Pe8WvR"
   },
   "outputs": [],
   "source": [
    "def handle_missing_values(data):\n",
    "    # We fill missing volumes with the sector's mean volume for the same date\n",
    "    for col in [col for col in data.columns if 'VOLUME' in col]:\n",
    "        # First, we try by filling with sector and date means\n",
    "        data[col] = data.groupby(['SECTOR', 'DATE'])[col].transform(\n",
    "            lambda x: x.fillna(x.mean())\n",
    "        )\n",
    "        # First fallback: we use the overall sector mean if date-level mean is NaN\n",
    "        data[col] = data.groupby('SECTOR')[col].transform(\n",
    "            lambda x: x.fillna(x.mean())\n",
    "        )\n",
    "        # Final fallback: we use the global mean for the column\n",
    "        data[col] = data[col].fillna(data[col].mean())\n",
    "\n",
    "    # Then, we fill missing returns with the average of adjacent columns\n",
    "    ret_columns = [col for col in data.columns if col.startswith('RET_')]\n",
    "    for col in ret_columns:\n",
    "        col_idx = int(col.split('_')[1])  # We extract the numeric suffix to consider the two (or only one) adjacents columns\n",
    "        prev_col = f'RET_{col_idx - 1}' if col_idx > 1 else None\n",
    "        next_col = f'RET_{col_idx + 1}' if f'RET_{col_idx + 1}' in ret_columns else None\n",
    "\n",
    "        if prev_col and next_col:\n",
    "            # We average both adjacent columns to get the filling value\n",
    "            data[col] = data[col].fillna(\n",
    "                (data[prev_col] + data[next_col]) / 2\n",
    "            )\n",
    "        if prev_col:\n",
    "            # We fallback to the previous column if it is available\n",
    "            data[col] = data[col].fillna(data[prev_col])\n",
    "        if next_col:\n",
    "            # We fallback to the next column if it is available\n",
    "            data[col] = data[col].fillna(data[next_col])\n",
    "\n",
    "        # Additional fallback: we use the mean return for the same sector and date\n",
    "        data[col] = data.groupby(['SECTOR', 'DATE'])[col].transform(\n",
    "            lambda x: x.fillna(x.mean())\n",
    "        )\n",
    "\n",
    "        # Final fallback: we use the global mean return for the column if none of the previous fallback worked\n",
    "        data[col] = data[col].fillna(data[col].mean())\n",
    "\n",
    "    return data\n",
    "\n",
    "# Let's preprocess train and test datasets to clean all missing values by filling them with the rules defined above\n",
    "train = handle_missing_values(train)\n",
    "test = handle_missing_values(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6aPsb7VrAAAp",
    "outputId": "b42066f0-0b50-4ebd-c0f6-7546d3c5e5bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE              0\n",
       "STOCK             0\n",
       "INDUSTRY          0\n",
       "INDUSTRY_GROUP    0\n",
       "SECTOR            0\n",
       "SUB_INDUSTRY      0\n",
       "RET_1             0\n",
       "VOLUME_1          0\n",
       "RET_2             0\n",
       "VOLUME_2          0\n",
       "RET_3             0\n",
       "VOLUME_3          0\n",
       "RET_4             0\n",
       "VOLUME_4          0\n",
       "RET_5             0\n",
       "VOLUME_5          0\n",
       "RET_6             0\n",
       "VOLUME_6          0\n",
       "RET_7             0\n",
       "VOLUME_7          0\n",
       "RET_8             0\n",
       "VOLUME_8          0\n",
       "RET_9             0\n",
       "VOLUME_9          0\n",
       "RET_10            0\n",
       "VOLUME_10         0\n",
       "RET_11            0\n",
       "VOLUME_11         0\n",
       "RET_12            0\n",
       "VOLUME_12         0\n",
       "RET_13            0\n",
       "VOLUME_13         0\n",
       "RET_14            0\n",
       "VOLUME_14         0\n",
       "RET_15            0\n",
       "VOLUME_15         0\n",
       "RET_16            0\n",
       "VOLUME_16         0\n",
       "RET_17            0\n",
       "VOLUME_17         0\n",
       "RET_18            0\n",
       "VOLUME_18         0\n",
       "RET_19            0\n",
       "VOLUME_19         0\n",
       "RET_20            0\n",
       "VOLUME_20         0\n",
       "RET               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We check that no columns have missing data.\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "8-YqXwmp2aPc",
    "outputId": "0ce74294-a8cf-40cf-8887-f12c8ef46be6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We could also print the matrix of correlation of the different features in order to understand what\n",
    "# conditionality we should consider between the features and the target variable.\n",
    "# The mean correlations are of course between Industry, Industry_group, Secotr and Sub_Industry and between volumes from days to days.\n",
    "correlation_matrix = train.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "O-LeC47n-g_J",
    "outputId": "3003ec84-338c-4cd4-8a6d-51d4661d493d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>STOCK</th>\n",
       "      <th>INDUSTRY</th>\n",
       "      <th>INDUSTRY_GROUP</th>\n",
       "      <th>SECTOR</th>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <th>RET_1</th>\n",
       "      <th>VOLUME_1</th>\n",
       "      <th>RET_2</th>\n",
       "      <th>VOLUME_2</th>\n",
       "      <th>...</th>\n",
       "      <th>VOLUME_16</th>\n",
       "      <th>RET_17</th>\n",
       "      <th>VOLUME_17</th>\n",
       "      <th>RET_18</th>\n",
       "      <th>VOLUME_18</th>\n",
       "      <th>RET_19</th>\n",
       "      <th>VOLUME_19</th>\n",
       "      <th>RET_20</th>\n",
       "      <th>VOLUME_20</th>\n",
       "      <th>RET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007430</td>\n",
       "      <td>-0.000682</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>-0.000714</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.071861</td>\n",
       "      <td>-0.002043</td>\n",
       "      <td>-0.014812</td>\n",
       "      <td>-0.001359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>-0.022053</td>\n",
       "      <td>0.003439</td>\n",
       "      <td>0.038149</td>\n",
       "      <td>-0.001369</td>\n",
       "      <td>-0.006160</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.014630</td>\n",
       "      <td>-0.003022</td>\n",
       "      <td>0.000473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STOCK</th>\n",
       "      <td>-0.007430</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005968</td>\n",
       "      <td>-0.007242</td>\n",
       "      <td>-0.007360</td>\n",
       "      <td>-0.003550</td>\n",
       "      <td>0.012167</td>\n",
       "      <td>-0.001812</td>\n",
       "      <td>0.010294</td>\n",
       "      <td>-0.001533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007950</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>-0.008535</td>\n",
       "      <td>-0.005169</td>\n",
       "      <td>-0.001661</td>\n",
       "      <td>-0.008116</td>\n",
       "      <td>-0.000406</td>\n",
       "      <td>-0.008244</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.002843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUSTRY</th>\n",
       "      <td>-0.000682</td>\n",
       "      <td>-0.005968</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994895</td>\n",
       "      <td>0.982486</td>\n",
       "      <td>0.997780</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.007507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>-0.001704</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>-0.007291</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>0.007436</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>-0.004050</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>-0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUSTRY_GROUP</th>\n",
       "      <td>-0.000720</td>\n",
       "      <td>-0.007242</td>\n",
       "      <td>0.994895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983174</td>\n",
       "      <td>0.994487</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>-0.007276</td>\n",
       "      <td>0.007114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>-0.001078</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>-0.006549</td>\n",
       "      <td>0.002497</td>\n",
       "      <td>0.006815</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>-0.003709</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>-0.000737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SECTOR</th>\n",
       "      <td>-0.000714</td>\n",
       "      <td>-0.007360</td>\n",
       "      <td>0.982486</td>\n",
       "      <td>0.983174</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980162</td>\n",
       "      <td>0.002268</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>-0.006845</td>\n",
       "      <td>0.006581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004153</td>\n",
       "      <td>-0.002495</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>-0.006047</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.008337</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>-0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.003550</td>\n",
       "      <td>0.997780</td>\n",
       "      <td>0.994487</td>\n",
       "      <td>0.980162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>0.004867</td>\n",
       "      <td>-0.006798</td>\n",
       "      <td>0.007151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>-0.007511</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>-0.001140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_1</th>\n",
       "      <td>-0.071861</td>\n",
       "      <td>0.012167</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.002268</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.102544</td>\n",
       "      <td>-0.031646</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.012867</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>-0.001625</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.005941</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>-0.008906</td>\n",
       "      <td>0.004223</td>\n",
       "      <td>-0.016771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_1</th>\n",
       "      <td>-0.002043</td>\n",
       "      <td>-0.001812</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>0.004867</td>\n",
       "      <td>0.102544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.042108</td>\n",
       "      <td>0.348768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006409</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>-0.006160</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>-0.007795</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>-0.007564</td>\n",
       "      <td>-0.002793</td>\n",
       "      <td>-0.005172</td>\n",
       "      <td>0.004570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_2</th>\n",
       "      <td>-0.014812</td>\n",
       "      <td>0.010294</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.007276</td>\n",
       "      <td>-0.006845</td>\n",
       "      <td>-0.006798</td>\n",
       "      <td>-0.031646</td>\n",
       "      <td>0.042108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.102553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003777</td>\n",
       "      <td>0.063303</td>\n",
       "      <td>-0.006418</td>\n",
       "      <td>0.034749</td>\n",
       "      <td>-0.003064</td>\n",
       "      <td>-0.040695</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>-0.055703</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>-0.009040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_2</th>\n",
       "      <td>-0.001359</td>\n",
       "      <td>-0.001533</td>\n",
       "      <td>0.007507</td>\n",
       "      <td>0.007114</td>\n",
       "      <td>0.006581</td>\n",
       "      <td>0.007151</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.348768</td>\n",
       "      <td>0.102553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005513</td>\n",
       "      <td>-0.000525</td>\n",
       "      <td>-0.005333</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>-0.007868</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>-0.009717</td>\n",
       "      <td>-0.000880</td>\n",
       "      <td>-0.007467</td>\n",
       "      <td>0.001029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_3</th>\n",
       "      <td>-0.006561</td>\n",
       "      <td>-0.001046</td>\n",
       "      <td>0.006591</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.007105</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>0.008624</td>\n",
       "      <td>0.039288</td>\n",
       "      <td>-0.129468</td>\n",
       "      <td>0.088724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>-0.059706</td>\n",
       "      <td>0.005565</td>\n",
       "      <td>-0.024429</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.014819</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.026155</td>\n",
       "      <td>0.003856</td>\n",
       "      <td>0.003393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_3</th>\n",
       "      <td>0.001054</td>\n",
       "      <td>-0.000370</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.196605</td>\n",
       "      <td>-0.006273</td>\n",
       "      <td>0.433842</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003285</td>\n",
       "      <td>-0.001265</td>\n",
       "      <td>-0.005712</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>-0.006771</td>\n",
       "      <td>-0.002225</td>\n",
       "      <td>-0.008392</td>\n",
       "      <td>-0.000480</td>\n",
       "      <td>-0.006821</td>\n",
       "      <td>0.001340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_4</th>\n",
       "      <td>-0.019103</td>\n",
       "      <td>0.010917</td>\n",
       "      <td>0.026482</td>\n",
       "      <td>0.025672</td>\n",
       "      <td>0.027120</td>\n",
       "      <td>0.026722</td>\n",
       "      <td>-0.013186</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.082156</td>\n",
       "      <td>0.012969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.046798</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.003315</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>-0.014457</td>\n",
       "      <td>-0.001515</td>\n",
       "      <td>-0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_4</th>\n",
       "      <td>0.002679</td>\n",
       "      <td>-0.001593</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.095092</td>\n",
       "      <td>-0.006080</td>\n",
       "      <td>0.118476</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007004</td>\n",
       "      <td>0.002904</td>\n",
       "      <td>-0.009478</td>\n",
       "      <td>0.001873</td>\n",
       "      <td>-0.008540</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>-0.011829</td>\n",
       "      <td>-0.001996</td>\n",
       "      <td>-0.010325</td>\n",
       "      <td>0.001702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_5</th>\n",
       "      <td>-0.030022</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>0.004353</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>-0.015876</td>\n",
       "      <td>0.015801</td>\n",
       "      <td>0.050625</td>\n",
       "      <td>0.014093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004685</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>-0.003915</td>\n",
       "      <td>-0.025209</td>\n",
       "      <td>-0.001428</td>\n",
       "      <td>0.016762</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>0.005037</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.003026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_5</th>\n",
       "      <td>0.000229</td>\n",
       "      <td>-0.004915</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.086921</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>0.098136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001991</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>-0.005556</td>\n",
       "      <td>0.003502</td>\n",
       "      <td>-0.007451</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>-0.008062</td>\n",
       "      <td>-0.003936</td>\n",
       "      <td>-0.008502</td>\n",
       "      <td>0.004182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_6</th>\n",
       "      <td>0.016622</td>\n",
       "      <td>-0.008094</td>\n",
       "      <td>-0.016651</td>\n",
       "      <td>-0.015946</td>\n",
       "      <td>-0.015704</td>\n",
       "      <td>-0.016998</td>\n",
       "      <td>0.012706</td>\n",
       "      <td>0.010247</td>\n",
       "      <td>0.052822</td>\n",
       "      <td>0.019104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.029676</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.051401</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>-0.001426</td>\n",
       "      <td>-0.003652</td>\n",
       "      <td>-0.009710</td>\n",
       "      <td>-0.003391</td>\n",
       "      <td>-0.004154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_6</th>\n",
       "      <td>-0.011231</td>\n",
       "      <td>-0.000806</td>\n",
       "      <td>0.004175</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.004549</td>\n",
       "      <td>0.005461</td>\n",
       "      <td>0.037766</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.049457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>-0.004844</td>\n",
       "      <td>-0.001358</td>\n",
       "      <td>-0.008631</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>-0.007772</td>\n",
       "      <td>-0.003619</td>\n",
       "      <td>-0.007977</td>\n",
       "      <td>-0.001305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_7</th>\n",
       "      <td>0.061408</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>-0.006694</td>\n",
       "      <td>-0.006783</td>\n",
       "      <td>-0.005941</td>\n",
       "      <td>-0.006770</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.006699</td>\n",
       "      <td>-0.070620</td>\n",
       "      <td>0.010384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007157</td>\n",
       "      <td>-0.032996</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>-0.009741</td>\n",
       "      <td>-0.001664</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>-0.001620</td>\n",
       "      <td>0.034671</td>\n",
       "      <td>-0.002847</td>\n",
       "      <td>0.005724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_7</th>\n",
       "      <td>-0.002474</td>\n",
       "      <td>-0.003363</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>0.007008</td>\n",
       "      <td>0.027920</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>0.046367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004342</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>-0.000183</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>-0.005903</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>-0.006789</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>-0.007121</td>\n",
       "      <td>-0.002836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_8</th>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.001648</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>0.004099</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.016075</td>\n",
       "      <td>0.008191</td>\n",
       "      <td>0.034937</td>\n",
       "      <td>0.006315</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>0.046411</td>\n",
       "      <td>-0.002969</td>\n",
       "      <td>0.071879</td>\n",
       "      <td>-0.003489</td>\n",
       "      <td>-0.019745</td>\n",
       "      <td>-0.001291</td>\n",
       "      <td>-0.017414</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>-0.008343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_8</th>\n",
       "      <td>0.003268</td>\n",
       "      <td>-0.003736</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.003767</td>\n",
       "      <td>-0.000454</td>\n",
       "      <td>0.015705</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>0.026251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009970</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>-0.002432</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>-0.001699</td>\n",
       "      <td>-0.005839</td>\n",
       "      <td>-0.000413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_9</th>\n",
       "      <td>0.036903</td>\n",
       "      <td>0.013099</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.004350</td>\n",
       "      <td>0.005456</td>\n",
       "      <td>0.005295</td>\n",
       "      <td>0.024832</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>0.010199</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.060655</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.030394</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>0.014961</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.009618</td>\n",
       "      <td>0.004317</td>\n",
       "      <td>-0.004089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_9</th>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>0.004008</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.011461</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.018079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014115</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.006565</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>-0.000774</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>-0.000283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_10</th>\n",
       "      <td>0.010094</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>-0.005709</td>\n",
       "      <td>-0.006494</td>\n",
       "      <td>-0.005682</td>\n",
       "      <td>-0.005912</td>\n",
       "      <td>-0.018643</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>-0.032602</td>\n",
       "      <td>0.008559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007718</td>\n",
       "      <td>-0.035449</td>\n",
       "      <td>-0.003507</td>\n",
       "      <td>-0.021541</td>\n",
       "      <td>-0.001485</td>\n",
       "      <td>0.005193</td>\n",
       "      <td>-0.000318</td>\n",
       "      <td>0.038535</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.005536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_10</th>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>0.008657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037039</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>0.033202</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.030565</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>-0.001980</td>\n",
       "      <td>-0.000954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_11</th>\n",
       "      <td>0.090203</td>\n",
       "      <td>0.006458</td>\n",
       "      <td>-0.000476</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>-0.001513</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.013136</td>\n",
       "      <td>0.002715</td>\n",
       "      <td>0.032175</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005000</td>\n",
       "      <td>0.032708</td>\n",
       "      <td>0.003173</td>\n",
       "      <td>0.012229</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>-0.044612</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>-0.003480</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>-0.002976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_11</th>\n",
       "      <td>-0.000276</td>\n",
       "      <td>-0.002778</td>\n",
       "      <td>-0.000647</td>\n",
       "      <td>-0.001071</td>\n",
       "      <td>-0.002637</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>-0.001676</td>\n",
       "      <td>-0.002396</td>\n",
       "      <td>0.003464</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046201</td>\n",
       "      <td>0.007151</td>\n",
       "      <td>0.045599</td>\n",
       "      <td>0.015996</td>\n",
       "      <td>0.035975</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>0.012514</td>\n",
       "      <td>0.005314</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.001460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_12</th>\n",
       "      <td>-0.001814</td>\n",
       "      <td>0.004050</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>-0.003166</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.022232</td>\n",
       "      <td>-0.000860</td>\n",
       "      <td>-0.006836</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>-0.000528</td>\n",
       "      <td>-0.002508</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>0.000840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_12</th>\n",
       "      <td>0.001360</td>\n",
       "      <td>-0.002228</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.002854</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>-0.001362</td>\n",
       "      <td>-0.003692</td>\n",
       "      <td>-0.000994</td>\n",
       "      <td>-0.002251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047139</td>\n",
       "      <td>0.010787</td>\n",
       "      <td>0.044589</td>\n",
       "      <td>0.008493</td>\n",
       "      <td>0.036800</td>\n",
       "      <td>0.008335</td>\n",
       "      <td>0.016597</td>\n",
       "      <td>0.007630</td>\n",
       "      <td>0.010281</td>\n",
       "      <td>-0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_13</th>\n",
       "      <td>0.061054</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.018212</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.091407</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>-0.070796</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>-0.061497</td>\n",
       "      <td>0.011277</td>\n",
       "      <td>0.033555</td>\n",
       "      <td>0.009823</td>\n",
       "      <td>0.081012</td>\n",
       "      <td>0.006301</td>\n",
       "      <td>-0.005725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_13</th>\n",
       "      <td>-0.001917</td>\n",
       "      <td>-0.004509</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>0.004552</td>\n",
       "      <td>0.003897</td>\n",
       "      <td>0.004515</td>\n",
       "      <td>-0.003456</td>\n",
       "      <td>-0.005275</td>\n",
       "      <td>-0.002001</td>\n",
       "      <td>-0.005747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110412</td>\n",
       "      <td>0.012621</td>\n",
       "      <td>0.127154</td>\n",
       "      <td>0.024190</td>\n",
       "      <td>0.122978</td>\n",
       "      <td>0.013148</td>\n",
       "      <td>0.030459</td>\n",
       "      <td>0.005460</td>\n",
       "      <td>0.024698</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_14</th>\n",
       "      <td>-0.051304</td>\n",
       "      <td>-0.000991</td>\n",
       "      <td>-0.009969</td>\n",
       "      <td>-0.008691</td>\n",
       "      <td>-0.009662</td>\n",
       "      <td>-0.010146</td>\n",
       "      <td>-0.017375</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>0.029457</td>\n",
       "      <td>-0.000258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004196</td>\n",
       "      <td>0.015178</td>\n",
       "      <td>-0.001275</td>\n",
       "      <td>-0.023708</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>-0.009060</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.012360</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>-0.001061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_14</th>\n",
       "      <td>-0.001015</td>\n",
       "      <td>-0.002733</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>0.006046</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>-0.000857</td>\n",
       "      <td>-0.005677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128681</td>\n",
       "      <td>0.011743</td>\n",
       "      <td>0.115497</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.086307</td>\n",
       "      <td>0.011821</td>\n",
       "      <td>0.047683</td>\n",
       "      <td>0.007521</td>\n",
       "      <td>0.046135</td>\n",
       "      <td>-0.002231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_15</th>\n",
       "      <td>-0.021518</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>-0.001918</td>\n",
       "      <td>-0.002048</td>\n",
       "      <td>-0.002646</td>\n",
       "      <td>-0.001254</td>\n",
       "      <td>0.037299</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>-0.009053</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.029908</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>-0.060844</td>\n",
       "      <td>0.003621</td>\n",
       "      <td>-0.025570</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>-0.003754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_15</th>\n",
       "      <td>0.000323</td>\n",
       "      <td>-0.000851</td>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.006741</td>\n",
       "      <td>0.006208</td>\n",
       "      <td>0.006904</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>-0.005982</td>\n",
       "      <td>-0.001650</td>\n",
       "      <td>-0.006189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214002</td>\n",
       "      <td>0.018406</td>\n",
       "      <td>0.171818</td>\n",
       "      <td>0.021253</td>\n",
       "      <td>0.154379</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.070886</td>\n",
       "      <td>0.013389</td>\n",
       "      <td>0.066814</td>\n",
       "      <td>-0.001126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_16</th>\n",
       "      <td>0.010937</td>\n",
       "      <td>0.008756</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>-0.006958</td>\n",
       "      <td>-0.004145</td>\n",
       "      <td>0.007241</td>\n",
       "      <td>-0.003092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082161</td>\n",
       "      <td>0.040568</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>0.007857</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>0.015790</td>\n",
       "      <td>-0.003046</td>\n",
       "      <td>0.014498</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.012007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_16</th>\n",
       "      <td>0.000821</td>\n",
       "      <td>-0.007950</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.004153</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>-0.006409</td>\n",
       "      <td>-0.003777</td>\n",
       "      <td>-0.005513</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039447</td>\n",
       "      <td>0.274801</td>\n",
       "      <td>0.020722</td>\n",
       "      <td>0.197949</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0.136882</td>\n",
       "      <td>0.014268</td>\n",
       "      <td>0.101621</td>\n",
       "      <td>-0.000325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_17</th>\n",
       "      <td>-0.022053</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>-0.001704</td>\n",
       "      <td>-0.001078</td>\n",
       "      <td>-0.002495</td>\n",
       "      <td>-0.001413</td>\n",
       "      <td>0.012867</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.063303</td>\n",
       "      <td>-0.000525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039447</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077760</td>\n",
       "      <td>0.016152</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>-0.022299</td>\n",
       "      <td>-0.002896</td>\n",
       "      <td>-0.025813</td>\n",
       "      <td>-0.001193</td>\n",
       "      <td>-0.013591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_17</th>\n",
       "      <td>0.003439</td>\n",
       "      <td>-0.008535</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>-0.006160</td>\n",
       "      <td>-0.006418</td>\n",
       "      <td>-0.005333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274801</td>\n",
       "      <td>0.077760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045754</td>\n",
       "      <td>0.471026</td>\n",
       "      <td>0.024765</td>\n",
       "      <td>0.278718</td>\n",
       "      <td>0.026317</td>\n",
       "      <td>0.170858</td>\n",
       "      <td>-0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_18</th>\n",
       "      <td>0.038149</td>\n",
       "      <td>-0.005169</td>\n",
       "      <td>-0.007291</td>\n",
       "      <td>-0.006549</td>\n",
       "      <td>-0.006047</td>\n",
       "      <td>-0.007511</td>\n",
       "      <td>-0.001625</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.034749</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020722</td>\n",
       "      <td>0.016152</td>\n",
       "      <td>0.045754</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.080469</td>\n",
       "      <td>-0.067606</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.027208</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.003537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_18</th>\n",
       "      <td>-0.001369</td>\n",
       "      <td>-0.001661</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>0.002497</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>-0.007795</td>\n",
       "      <td>-0.003064</td>\n",
       "      <td>-0.007868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197949</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.471026</td>\n",
       "      <td>0.080469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037643</td>\n",
       "      <td>0.435419</td>\n",
       "      <td>0.029255</td>\n",
       "      <td>0.224449</td>\n",
       "      <td>0.001120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_19</th>\n",
       "      <td>-0.006160</td>\n",
       "      <td>-0.008116</td>\n",
       "      <td>0.007436</td>\n",
       "      <td>0.006815</td>\n",
       "      <td>0.008337</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>0.005941</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>-0.040695</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>-0.022299</td>\n",
       "      <td>0.024765</td>\n",
       "      <td>-0.067606</td>\n",
       "      <td>0.037643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104354</td>\n",
       "      <td>0.067950</td>\n",
       "      <td>0.008451</td>\n",
       "      <td>0.003338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_19</th>\n",
       "      <td>0.000181</td>\n",
       "      <td>-0.000406</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>-0.007564</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>-0.009717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136882</td>\n",
       "      <td>-0.002896</td>\n",
       "      <td>0.278718</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>0.435419</td>\n",
       "      <td>0.104354</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071053</td>\n",
       "      <td>0.414368</td>\n",
       "      <td>0.000718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET_20</th>\n",
       "      <td>0.014630</td>\n",
       "      <td>-0.008244</td>\n",
       "      <td>-0.004050</td>\n",
       "      <td>-0.003709</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>-0.008906</td>\n",
       "      <td>-0.002793</td>\n",
       "      <td>-0.055703</td>\n",
       "      <td>-0.000880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014268</td>\n",
       "      <td>-0.025813</td>\n",
       "      <td>0.026317</td>\n",
       "      <td>-0.027208</td>\n",
       "      <td>0.029255</td>\n",
       "      <td>0.067950</td>\n",
       "      <td>0.071053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194484</td>\n",
       "      <td>-0.002916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOLUME_20</th>\n",
       "      <td>-0.003022</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.004223</td>\n",
       "      <td>-0.005172</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>-0.007467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101621</td>\n",
       "      <td>-0.001193</td>\n",
       "      <td>0.170858</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.224449</td>\n",
       "      <td>0.008451</td>\n",
       "      <td>0.414368</td>\n",
       "      <td>0.194484</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET</th>\n",
       "      <td>0.000473</td>\n",
       "      <td>-0.002843</td>\n",
       "      <td>-0.000997</td>\n",
       "      <td>-0.000737</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>-0.001140</td>\n",
       "      <td>-0.016771</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>-0.009040</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000325</td>\n",
       "      <td>-0.013591</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>0.003537</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.003338</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>-0.002916</td>\n",
       "      <td>-0.001204</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    DATE     STOCK  INDUSTRY  INDUSTRY_GROUP    SECTOR  \\\n",
       "DATE            1.000000 -0.007430 -0.000682       -0.000720 -0.000714   \n",
       "STOCK          -0.007430  1.000000 -0.005968       -0.007242 -0.007360   \n",
       "INDUSTRY       -0.000682 -0.005968  1.000000        0.994895  0.982486   \n",
       "INDUSTRY_GROUP -0.000720 -0.007242  0.994895        1.000000  0.983174   \n",
       "SECTOR         -0.000714 -0.007360  0.982486        0.983174  1.000000   \n",
       "SUB_INDUSTRY   -0.000639 -0.003550  0.997780        0.994487  0.980162   \n",
       "RET_1          -0.071861  0.012167  0.001818        0.002675  0.002268   \n",
       "VOLUME_1       -0.002043 -0.001812  0.005092        0.004850  0.004396   \n",
       "RET_2          -0.014812  0.010294 -0.006818       -0.007276 -0.006845   \n",
       "VOLUME_2       -0.001359 -0.001533  0.007507        0.007114  0.006581   \n",
       "RET_3          -0.006561 -0.001046  0.006591        0.006470  0.007105   \n",
       "VOLUME_3        0.001054 -0.000370  0.002525        0.002311  0.001132   \n",
       "RET_4          -0.019103  0.010917  0.026482        0.025672  0.027120   \n",
       "VOLUME_4        0.002679 -0.001593  0.001904        0.001744  0.000244   \n",
       "RET_5          -0.030022  0.005773  0.004232        0.004572  0.004353   \n",
       "VOLUME_5        0.000229 -0.004915  0.002903        0.002675  0.000565   \n",
       "RET_6           0.016622 -0.008094 -0.016651       -0.015946 -0.015704   \n",
       "VOLUME_6       -0.011231 -0.000806  0.004175        0.004105  0.001573   \n",
       "RET_7           0.061408  0.001742 -0.006694       -0.006783 -0.005941   \n",
       "VOLUME_7       -0.002474 -0.003363  0.002986        0.002639  0.000526   \n",
       "RET_8          -0.000031 -0.001648  0.004618        0.004099  0.004631   \n",
       "VOLUME_8        0.003268 -0.003736  0.003131        0.003009  0.000730   \n",
       "RET_9           0.036903  0.013099  0.005148        0.004350  0.005456   \n",
       "VOLUME_9        0.000042  0.002343  0.004008        0.003564  0.001838   \n",
       "RET_10          0.010094  0.001889 -0.005709       -0.006494 -0.005682   \n",
       "VOLUME_10       0.000802  0.000946  0.002435        0.002278  0.000358   \n",
       "RET_11          0.090203  0.006458 -0.000476       -0.000201 -0.001513   \n",
       "VOLUME_11      -0.000276 -0.002778 -0.000647       -0.001071 -0.002637   \n",
       "RET_12         -0.001814  0.004050  0.002151        0.002422  0.002444   \n",
       "VOLUME_12       0.001360 -0.002228  0.003049        0.002854  0.002713   \n",
       "RET_13          0.061054  0.009004  0.002351        0.001887  0.002344   \n",
       "VOLUME_13      -0.001917 -0.004509  0.004618        0.004552  0.003897   \n",
       "RET_14         -0.051304 -0.000991 -0.009969       -0.008691 -0.009662   \n",
       "VOLUME_14      -0.001015 -0.002733  0.005918        0.006046  0.006015   \n",
       "RET_15         -0.021518  0.008362 -0.001918       -0.002048 -0.002646   \n",
       "VOLUME_15       0.000323 -0.000851  0.006844        0.006741  0.006208   \n",
       "RET_16          0.010937  0.008756  0.001084        0.000781  0.001335   \n",
       "VOLUME_16       0.000821 -0.007950  0.004165        0.004182  0.004153   \n",
       "RET_17         -0.022053  0.015000 -0.001704       -0.001078 -0.002495   \n",
       "VOLUME_17       0.003439 -0.008535  0.003219        0.002762  0.002888   \n",
       "RET_18          0.038149 -0.005169 -0.007291       -0.006549 -0.006047   \n",
       "VOLUME_18      -0.001369 -0.001661  0.002808        0.002497  0.001976   \n",
       "RET_19         -0.006160 -0.008116  0.007436        0.006815  0.008337   \n",
       "VOLUME_19       0.000181 -0.000406  0.002773        0.002249  0.001433   \n",
       "RET_20          0.014630 -0.008244 -0.004050       -0.003709 -0.004185   \n",
       "VOLUME_20      -0.003022 -0.000015  0.000741        0.000720  0.000337   \n",
       "RET             0.000473 -0.002843 -0.000997       -0.000737 -0.000137   \n",
       "\n",
       "                SUB_INDUSTRY     RET_1  VOLUME_1     RET_2  VOLUME_2  ...  \\\n",
       "DATE               -0.000639 -0.071861 -0.002043 -0.014812 -0.001359  ...   \n",
       "STOCK              -0.003550  0.012167 -0.001812  0.010294 -0.001533  ...   \n",
       "INDUSTRY            0.997780  0.001818  0.005092 -0.006818  0.007507  ...   \n",
       "INDUSTRY_GROUP      0.994487  0.002675  0.004850 -0.007276  0.007114  ...   \n",
       "SECTOR              0.980162  0.002268  0.004396 -0.006845  0.006581  ...   \n",
       "SUB_INDUSTRY        1.000000  0.002343  0.004867 -0.006798  0.007151  ...   \n",
       "RET_1               0.002343  1.000000  0.102544 -0.031646  0.002632  ...   \n",
       "VOLUME_1            0.004867  0.102544  1.000000  0.042108  0.348768  ...   \n",
       "RET_2              -0.006798 -0.031646  0.042108  1.000000  0.102553  ...   \n",
       "VOLUME_2            0.007151  0.002632  0.348768  0.102553  1.000000  ...   \n",
       "RET_3               0.006182  0.008624  0.039288 -0.129468  0.088724  ...   \n",
       "VOLUME_3            0.002386  0.000277  0.196605 -0.006273  0.433842  ...   \n",
       "RET_4               0.026722 -0.013186  0.007100  0.082156  0.012969  ...   \n",
       "VOLUME_4            0.002000  0.001576  0.095092 -0.006080  0.118476  ...   \n",
       "RET_5               0.004361 -0.015876  0.015801  0.050625  0.014093  ...   \n",
       "VOLUME_5            0.003252  0.000284  0.086921  0.002211  0.098136  ...   \n",
       "RET_6              -0.016998  0.012706  0.010247  0.052822  0.019104  ...   \n",
       "VOLUME_6            0.004549  0.005461  0.037766  0.006030  0.049457  ...   \n",
       "RET_7              -0.006770  0.000132  0.006699 -0.070620  0.010384  ...   \n",
       "VOLUME_7            0.003352  0.007008  0.027920  0.001171  0.046367  ...   \n",
       "RET_8               0.005172  0.016075  0.008191  0.034937  0.006315  ...   \n",
       "VOLUME_8            0.003767 -0.000454  0.015705 -0.000125  0.026251  ...   \n",
       "RET_9               0.005295  0.024832  0.003105  0.010199  0.003077  ...   \n",
       "VOLUME_9            0.004412  0.001890  0.011461  0.000561  0.018079  ...   \n",
       "RET_10             -0.005912 -0.018643  0.004327 -0.032602  0.008559  ...   \n",
       "VOLUME_10           0.003019  0.003126  0.002326 -0.000256  0.008657  ...   \n",
       "RET_11             -0.000015 -0.013136  0.002715  0.032175  0.001160  ...   \n",
       "VOLUME_11          -0.000453 -0.001676 -0.002396  0.003464  0.000129  ...   \n",
       "RET_12              0.002311  0.001763  0.000150 -0.003166  0.000429  ...   \n",
       "VOLUME_12           0.002801 -0.001362 -0.003692 -0.000994 -0.002251  ...   \n",
       "RET_13              0.002317  0.018212 -0.000133 -0.091407  0.001375  ...   \n",
       "VOLUME_13           0.004515 -0.003456 -0.005275 -0.002001 -0.005747  ...   \n",
       "RET_14             -0.010146 -0.017375  0.001954  0.029457 -0.000258  ...   \n",
       "VOLUME_14           0.005784  0.000209  0.006525 -0.000857 -0.005677  ...   \n",
       "RET_15             -0.001254  0.037299  0.000434 -0.009053  0.002344  ...   \n",
       "VOLUME_15           0.006904  0.002466 -0.005982 -0.001650 -0.006189  ...   \n",
       "RET_16              0.000795 -0.006958 -0.004145  0.007241 -0.003092  ...   \n",
       "VOLUME_16           0.004152  0.003142 -0.006409 -0.003777 -0.005513  ...   \n",
       "RET_17             -0.001413  0.012867  0.001204  0.063303 -0.000525  ...   \n",
       "VOLUME_17           0.002935  0.002335 -0.006160 -0.006418 -0.005333  ...   \n",
       "RET_18             -0.007511 -0.001625  0.001017  0.034749 -0.000713  ...   \n",
       "VOLUME_18           0.002884  0.000933 -0.007795 -0.003064 -0.007868  ...   \n",
       "RET_19              0.007095  0.005941  0.000430 -0.040695  0.001174  ...   \n",
       "VOLUME_19           0.002844  0.001291 -0.007564  0.000827 -0.009717  ...   \n",
       "RET_20             -0.003883 -0.008906 -0.002793 -0.055703 -0.000880  ...   \n",
       "VOLUME_20           0.000634  0.004223 -0.005172  0.003039 -0.007467  ...   \n",
       "RET                -0.001140 -0.016771  0.004570 -0.009040  0.001029  ...   \n",
       "\n",
       "                VOLUME_16    RET_17  VOLUME_17    RET_18  VOLUME_18    RET_19  \\\n",
       "DATE             0.000821 -0.022053   0.003439  0.038149  -0.001369 -0.006160   \n",
       "STOCK           -0.007950  0.015000  -0.008535 -0.005169  -0.001661 -0.008116   \n",
       "INDUSTRY         0.004165 -0.001704   0.003219 -0.007291   0.002808  0.007436   \n",
       "INDUSTRY_GROUP   0.004182 -0.001078   0.002762 -0.006549   0.002497  0.006815   \n",
       "SECTOR           0.004153 -0.002495   0.002888 -0.006047   0.001976  0.008337   \n",
       "SUB_INDUSTRY     0.004152 -0.001413   0.002935 -0.007511   0.002884  0.007095   \n",
       "RET_1            0.003142  0.012867   0.002335 -0.001625   0.000933  0.005941   \n",
       "VOLUME_1        -0.006409  0.001204  -0.006160  0.001017  -0.007795  0.000430   \n",
       "RET_2           -0.003777  0.063303  -0.006418  0.034749  -0.003064 -0.040695   \n",
       "VOLUME_2        -0.005513 -0.000525  -0.005333 -0.000713  -0.007868  0.001174   \n",
       "RET_3            0.000576 -0.059706   0.005565 -0.024429   0.001802  0.014819   \n",
       "VOLUME_3        -0.003285 -0.001265  -0.005712  0.000595  -0.006771 -0.002225   \n",
       "RET_4            0.003964  0.046798  -0.000039  0.003315   0.003570  0.012073   \n",
       "VOLUME_4        -0.007004  0.002904  -0.009478  0.001873  -0.008540  0.002776   \n",
       "RET_5            0.004685  0.006159  -0.003915 -0.025209  -0.001428  0.016762   \n",
       "VOLUME_5        -0.001991  0.001860  -0.005556  0.003502  -0.007451  0.001322   \n",
       "RET_6            0.000156  0.029676   0.001928  0.051401   0.001082 -0.001426   \n",
       "VOLUME_6         0.000927  0.003701  -0.004844 -0.001358  -0.008631  0.000335   \n",
       "RET_7            0.007157 -0.032996   0.001671 -0.009741  -0.001664  0.003961   \n",
       "VOLUME_7         0.004342  0.004902  -0.000183  0.000679  -0.005903  0.001574   \n",
       "RET_8           -0.000262  0.046411  -0.002969  0.071879  -0.003489 -0.019745   \n",
       "VOLUME_8         0.009970  0.002424   0.000750  0.000121  -0.002432  0.001906   \n",
       "RET_9            0.001129  0.060655   0.002669  0.030394   0.002946  0.014961   \n",
       "VOLUME_9         0.014115  0.002903   0.006565  0.000941  -0.000774  0.005944   \n",
       "RET_10           0.007718 -0.035449  -0.003507 -0.021541  -0.001485  0.005193   \n",
       "VOLUME_10        0.037039  0.003367   0.033202  0.008100   0.030565  0.002273   \n",
       "RET_11          -0.005000  0.032708   0.003173  0.012229   0.000361 -0.044612   \n",
       "VOLUME_11        0.046201  0.007151   0.045599  0.015996   0.035975  0.003505   \n",
       "RET_12           0.002196  0.022232  -0.000860 -0.006836  -0.000315  0.001473   \n",
       "VOLUME_12        0.047139  0.010787   0.044589  0.008493   0.036800  0.008335   \n",
       "RET_13           0.000905 -0.070796   0.007576 -0.061497   0.011277  0.033555   \n",
       "VOLUME_13        0.110412  0.012621   0.127154  0.024190   0.122978  0.013148   \n",
       "RET_14          -0.004196  0.015178  -0.001275 -0.023708   0.002154 -0.009060   \n",
       "VOLUME_14        0.128681  0.011743   0.115497  0.018822   0.086307  0.011821   \n",
       "RET_15           0.000167  0.001984   0.001663  0.029908   0.001844 -0.060844   \n",
       "VOLUME_15        0.214002  0.018406   0.171818  0.021253   0.154379  0.013123   \n",
       "RET_16           0.082161  0.040568   0.003631  0.007857   0.006660  0.015790   \n",
       "VOLUME_16        1.000000  0.039447   0.274801  0.020722   0.197949  0.016340   \n",
       "RET_17           0.039447  1.000000   0.077760  0.016152   0.003541 -0.022299   \n",
       "VOLUME_17        0.274801  0.077760   1.000000  0.045754   0.471026  0.024765   \n",
       "RET_18           0.020722  0.016152   0.045754  1.000000   0.080469 -0.067606   \n",
       "VOLUME_18        0.197949  0.003541   0.471026  0.080469   1.000000  0.037643   \n",
       "RET_19           0.016340 -0.022299   0.024765 -0.067606   0.037643  1.000000   \n",
       "VOLUME_19        0.136882 -0.002896   0.278718  0.003777   0.435419  0.104354   \n",
       "RET_20           0.014268 -0.025813   0.026317 -0.027208   0.029255  0.067950   \n",
       "VOLUME_20        0.101621 -0.001193   0.170858  0.000399   0.224449  0.008451   \n",
       "RET             -0.000325 -0.013591  -0.000999  0.003537   0.001120  0.003338   \n",
       "\n",
       "                VOLUME_19    RET_20  VOLUME_20       RET  \n",
       "DATE             0.000181  0.014630  -0.003022  0.000473  \n",
       "STOCK           -0.000406 -0.008244  -0.000015 -0.002843  \n",
       "INDUSTRY         0.002773 -0.004050   0.000741 -0.000997  \n",
       "INDUSTRY_GROUP   0.002249 -0.003709   0.000720 -0.000737  \n",
       "SECTOR           0.001433 -0.004185   0.000337 -0.000137  \n",
       "SUB_INDUSTRY     0.002844 -0.003883   0.000634 -0.001140  \n",
       "RET_1            0.001291 -0.008906   0.004223 -0.016771  \n",
       "VOLUME_1        -0.007564 -0.002793  -0.005172  0.004570  \n",
       "RET_2            0.000827 -0.055703   0.003039 -0.009040  \n",
       "VOLUME_2        -0.009717 -0.000880  -0.007467  0.001029  \n",
       "RET_3            0.003958  0.026155   0.003856  0.003393  \n",
       "VOLUME_3        -0.008392 -0.000480  -0.006821  0.001340  \n",
       "RET_4            0.000720 -0.014457  -0.001515 -0.005100  \n",
       "VOLUME_4        -0.011829 -0.001996  -0.010325  0.001702  \n",
       "RET_5            0.001555  0.005037   0.000050  0.003026  \n",
       "VOLUME_5        -0.008062 -0.003936  -0.008502  0.004182  \n",
       "RET_6           -0.003652 -0.009710  -0.003391 -0.004154  \n",
       "VOLUME_6        -0.007772 -0.003619  -0.007977 -0.001305  \n",
       "RET_7           -0.001620  0.034671  -0.002847  0.005724  \n",
       "VOLUME_7        -0.006789 -0.000080  -0.007121 -0.002836  \n",
       "RET_8           -0.001291 -0.017414  -0.000728 -0.008343  \n",
       "VOLUME_8         0.001642 -0.001699  -0.005839 -0.000413  \n",
       "RET_9            0.001786  0.009618   0.004317 -0.004089  \n",
       "VOLUME_9         0.000083 -0.000094   0.000058 -0.000283  \n",
       "RET_10          -0.000318  0.038535   0.001961  0.005536  \n",
       "VOLUME_10        0.000376  0.003913  -0.001980 -0.000954  \n",
       "RET_11           0.001587 -0.003480   0.000241 -0.002976  \n",
       "VOLUME_11        0.012514  0.005314   0.001417  0.001460  \n",
       "RET_12          -0.000528 -0.002508  -0.000145  0.000840  \n",
       "VOLUME_12        0.016597  0.007630   0.010281 -0.000034  \n",
       "RET_13           0.009823  0.081012   0.006301 -0.005725  \n",
       "VOLUME_13        0.030459  0.005460   0.024698  0.000109  \n",
       "RET_14           0.000053 -0.012360   0.002813 -0.001061  \n",
       "VOLUME_14        0.047683  0.007521   0.046135 -0.002231  \n",
       "RET_15           0.003621 -0.025570   0.001961 -0.003754  \n",
       "VOLUME_15        0.070886  0.013389   0.066814 -0.001126  \n",
       "RET_16          -0.003046  0.014498  -0.000100 -0.012007  \n",
       "VOLUME_16        0.136882  0.014268   0.101621 -0.000325  \n",
       "RET_17          -0.002896 -0.025813  -0.001193 -0.013591  \n",
       "VOLUME_17        0.278718  0.026317   0.170858 -0.000999  \n",
       "RET_18           0.003777 -0.027208   0.000399  0.003537  \n",
       "VOLUME_18        0.435419  0.029255   0.224449  0.001120  \n",
       "RET_19           0.104354  0.067950   0.008451  0.003338  \n",
       "VOLUME_19        1.000000  0.071053   0.414368  0.000718  \n",
       "RET_20           0.071053  1.000000   0.194484 -0.002916  \n",
       "VOLUME_20        0.414368  0.194484   1.000000 -0.001204  \n",
       "RET              0.000718 -0.002916  -0.001204  1.000000  \n",
       "\n",
       "[47 rows x 47 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bYAV0yF_WIK"
   },
   "source": [
    "We will here try to generate a feature that describe the mean, min, max and standard deviation of `RET_1` to `RET_5` conditionally to the `SECTOR` and the `DATE`, the `INDUSTRY_GROUP` and the `DATE` and the `INDUSTRY` and the `DATE` to create more conditional features and hopefully improve the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ckydo5vz1i--",
    "outputId": "115d26c1-51f1-49de-f8bf-791c05368073"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
      "<ipython-input-14-3b3467580c96>:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[name] = data.groupby(gb_feature)[feat].transform(stat)\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering\n",
    "new_features = []\n",
    "\n",
    "# Conditional aggregated features : we added the Industry_group\n",
    "shifts = [1, 2, 3, 4, 5]\n",
    "statistics = ['mean','min','max','std']  # The type of statistic\n",
    "gb_features = [['SECTOR', 'DATE'], ['INDUSTRY_GROUP', 'DATE'], ['INDUSTRY', 'DATE']]\n",
    "target_feature = ['RET', 'VOLUME']\n",
    "# The shift represents the number of days before the day we consider.\n",
    "for target in target_feature:\n",
    "  for gb_feature in gb_features:\n",
    "    for shift in shifts:\n",
    "        for stat in statistics:\n",
    "            tmp_name = '_'.join(gb_feature)\n",
    "            name = f'{target}_{shift}_{tmp_name}_{stat}'\n",
    "            feat = f'{target}_{shift}'\n",
    "            new_features.append(name)\n",
    "            for data in [train, test]:\n",
    "                data[name] = data.groupby(gb_feature)[feat].transform(stat)\n",
    "                data[name] = data[name].fillna(method='bfill').fillna(method='ffill')\n",
    "                # Here we fill some missing values of the standard deviation that could create some NaN values in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsqVKbmI7Krh"
   },
   "source": [
    "# Feature Selection Bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "hsOF7Rkc7Jz6",
    "outputId": "720036a0-5fe7-45e2-c5b4-9217d44205f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RET_1</th>\n",
       "      <th>RET_2</th>\n",
       "      <th>RET_3</th>\n",
       "      <th>RET_4</th>\n",
       "      <th>RET_5</th>\n",
       "      <th>VOLUME_1</th>\n",
       "      <th>VOLUME_2</th>\n",
       "      <th>VOLUME_3</th>\n",
       "      <th>VOLUME_4</th>\n",
       "      <th>VOLUME_5</th>\n",
       "      <th>...</th>\n",
       "      <th>VOLUME_3_INDUSTRY_DATE_max</th>\n",
       "      <th>VOLUME_3_INDUSTRY_DATE_std</th>\n",
       "      <th>VOLUME_4_INDUSTRY_DATE_mean</th>\n",
       "      <th>VOLUME_4_INDUSTRY_DATE_min</th>\n",
       "      <th>VOLUME_4_INDUSTRY_DATE_max</th>\n",
       "      <th>VOLUME_4_INDUSTRY_DATE_std</th>\n",
       "      <th>VOLUME_5_INDUSTRY_DATE_mean</th>\n",
       "      <th>VOLUME_5_INDUSTRY_DATE_min</th>\n",
       "      <th>VOLUME_5_INDUSTRY_DATE_max</th>\n",
       "      <th>VOLUME_5_INDUSTRY_DATE_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.015748</td>\n",
       "      <td>-0.015504</td>\n",
       "      <td>0.010972</td>\n",
       "      <td>-0.014672</td>\n",
       "      <td>0.016483</td>\n",
       "      <td>0.147931</td>\n",
       "      <td>0.179183</td>\n",
       "      <td>0.033832</td>\n",
       "      <td>-0.362868</td>\n",
       "      <td>-0.972920</td>\n",
       "      <td>...</td>\n",
       "      <td>1.426388</td>\n",
       "      <td>0.743043</td>\n",
       "      <td>-0.215966</td>\n",
       "      <td>-0.746342</td>\n",
       "      <td>0.597689</td>\n",
       "      <td>0.431639</td>\n",
       "      <td>-0.455239</td>\n",
       "      <td>-0.972920</td>\n",
       "      <td>-0.065641</td>\n",
       "      <td>0.314736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003984</td>\n",
       "      <td>-0.090580</td>\n",
       "      <td>0.018826</td>\n",
       "      <td>-0.025540</td>\n",
       "      <td>-0.038062</td>\n",
       "      <td>0.009725</td>\n",
       "      <td>-0.039260</td>\n",
       "      <td>-0.189169</td>\n",
       "      <td>-0.262853</td>\n",
       "      <td>-0.209702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927769</td>\n",
       "      <td>0.398013</td>\n",
       "      <td>-0.302671</td>\n",
       "      <td>-1.280212</td>\n",
       "      <td>8.755558</td>\n",
       "      <td>1.190099</td>\n",
       "      <td>-0.188548</td>\n",
       "      <td>-1.190652</td>\n",
       "      <td>8.413029</td>\n",
       "      <td>1.252840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.058896</td>\n",
       "      <td>-0.009042</td>\n",
       "      <td>0.024852</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>-0.096282</td>\n",
       "      <td>0.084771</td>\n",
       "      <td>-0.298777</td>\n",
       "      <td>-0.157421</td>\n",
       "      <td>0.091455</td>\n",
       "      <td>...</td>\n",
       "      <td>5.539782</td>\n",
       "      <td>0.950714</td>\n",
       "      <td>-0.152875</td>\n",
       "      <td>-1.166698</td>\n",
       "      <td>6.917564</td>\n",
       "      <td>1.043218</td>\n",
       "      <td>-0.113151</td>\n",
       "      <td>-1.189309</td>\n",
       "      <td>8.762385</td>\n",
       "      <td>1.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.031298</td>\n",
       "      <td>0.007756</td>\n",
       "      <td>-0.004632</td>\n",
       "      <td>-0.019677</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>-0.429540</td>\n",
       "      <td>-0.089919</td>\n",
       "      <td>-0.639737</td>\n",
       "      <td>-0.940163</td>\n",
       "      <td>-0.882464</td>\n",
       "      <td>...</td>\n",
       "      <td>1.285789</td>\n",
       "      <td>0.495449</td>\n",
       "      <td>-0.214578</td>\n",
       "      <td>-0.967795</td>\n",
       "      <td>1.770693</td>\n",
       "      <td>0.548485</td>\n",
       "      <td>-0.557493</td>\n",
       "      <td>-1.111108</td>\n",
       "      <td>0.351711</td>\n",
       "      <td>0.341009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.027273</td>\n",
       "      <td>-0.039302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>-0.847155</td>\n",
       "      <td>-0.943033</td>\n",
       "      <td>-1.180629</td>\n",
       "      <td>-1.313896</td>\n",
       "      <td>-1.204398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.723614</td>\n",
       "      <td>0.421935</td>\n",
       "      <td>-0.189692</td>\n",
       "      <td>-1.313896</td>\n",
       "      <td>2.234868</td>\n",
       "      <td>0.714730</td>\n",
       "      <td>-0.220659</td>\n",
       "      <td>-1.404361</td>\n",
       "      <td>3.457331</td>\n",
       "      <td>0.858263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       RET_1     RET_2     RET_3     RET_4     RET_5  VOLUME_1  VOLUME_2  \\\n",
       "ID                                                                         \n",
       "0  -0.015748 -0.015504  0.010972 -0.014672  0.016483  0.147931  0.179183   \n",
       "1   0.003984 -0.090580  0.018826 -0.025540 -0.038062  0.009725 -0.039260   \n",
       "2   0.000440 -0.058896 -0.009042  0.024852  0.009354 -0.096282  0.084771   \n",
       "3   0.031298  0.007756 -0.004632 -0.019677  0.003544 -0.429540 -0.089919   \n",
       "4   0.027273 -0.039302  0.000000  0.000000  0.022321 -0.847155 -0.943033   \n",
       "\n",
       "    VOLUME_3  VOLUME_4  VOLUME_5  ...  VOLUME_3_INDUSTRY_DATE_max  \\\n",
       "ID                                ...                               \n",
       "0   0.033832 -0.362868 -0.972920  ...                    1.426388   \n",
       "1  -0.189169 -0.262853 -0.209702  ...                    0.927769   \n",
       "2  -0.298777 -0.157421  0.091455  ...                    5.539782   \n",
       "3  -0.639737 -0.940163 -0.882464  ...                    1.285789   \n",
       "4  -1.180629 -1.313896 -1.204398  ...                    0.723614   \n",
       "\n",
       "    VOLUME_3_INDUSTRY_DATE_std  VOLUME_4_INDUSTRY_DATE_mean  \\\n",
       "ID                                                            \n",
       "0                     0.743043                    -0.215966   \n",
       "1                     0.398013                    -0.302671   \n",
       "2                     0.950714                    -0.152875   \n",
       "3                     0.495449                    -0.214578   \n",
       "4                     0.421935                    -0.189692   \n",
       "\n",
       "    VOLUME_4_INDUSTRY_DATE_min  VOLUME_4_INDUSTRY_DATE_max  \\\n",
       "ID                                                           \n",
       "0                    -0.746342                    0.597689   \n",
       "1                    -1.280212                    8.755558   \n",
       "2                    -1.166698                    6.917564   \n",
       "3                    -0.967795                    1.770693   \n",
       "4                    -1.313896                    2.234868   \n",
       "\n",
       "    VOLUME_4_INDUSTRY_DATE_std  VOLUME_5_INDUSTRY_DATE_mean  \\\n",
       "ID                                                            \n",
       "0                     0.431639                    -0.455239   \n",
       "1                     1.190099                    -0.188548   \n",
       "2                     1.043218                    -0.113151   \n",
       "3                     0.548485                    -0.557493   \n",
       "4                     0.714730                    -0.220659   \n",
       "\n",
       "    VOLUME_5_INDUSTRY_DATE_min  VOLUME_5_INDUSTRY_DATE_max  \\\n",
       "ID                                                           \n",
       "0                    -0.972920                   -0.065641   \n",
       "1                    -1.190652                    8.413029   \n",
       "2                    -1.189309                    8.762385   \n",
       "3                    -1.111108                    0.351711   \n",
       "4                    -1.404361                    3.457331   \n",
       "\n",
       "    VOLUME_5_INDUSTRY_DATE_std  \n",
       "ID                              \n",
       "0                     0.314736  \n",
       "1                     1.252840  \n",
       "2                     1.303000  \n",
       "3                     0.341009  \n",
       "4                     0.858263  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'RET'\n",
    "\n",
    "n_shifts = 5\n",
    "features = ['RET_%d' % (i + 1) for i in range(n_shifts)] # features = ['RET_1', 'RET_2', 'RET_3', 'RET_4', 'RET_5']\n",
    "features += ['VOLUME_%d' % (i + 1) for i in range(n_shifts)] # features = ['RET_1', 'RET_2', 'RET_3', 'RET_4', 'RET_5', 'VOLUME_1', 'VOLUME_2', 'VOLUME_3', 'VOLUME_4', 'VOLUME_5']\n",
    "features += new_features\n",
    "train[features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "fx6olq04EgZi",
    "outputId": "e27a64f8-d734-4b09-f701-2ea159473198"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RET_1                          0\n",
       "RET_2                          0\n",
       "RET_3                          0\n",
       "RET_4                          0\n",
       "RET_5                          0\n",
       "                              ..\n",
       "VOLUME_4_INDUSTRY_DATE_std     0\n",
       "VOLUME_5_INDUSTRY_DATE_mean    0\n",
       "VOLUME_5_INDUSTRY_DATE_min     0\n",
       "VOLUME_5_INDUSTRY_DATE_max     0\n",
       "VOLUME_5_INDUSTRY_DATE_std     0\n",
       "Length: 130, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[features].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ytp3MHwR5SBJ"
   },
   "source": [
    "## Model and local score Bis\n",
    "\n",
    "We are going to tune the hyperparameters of the Random Forest in order to improve the accuracy of the model.\n",
    "\n",
    "Other improvements such as changing the features considered and trying other models (such as Support Vector Models, eXtreme Gradient Boosting etc) could also be implemented as next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRSAogkg4wwJ"
   },
   "outputs": [],
   "source": [
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "\n",
    "train_dates = train['DATE'].unique()\n",
    "test_dates = test['DATE'].unique()\n",
    "\n",
    "n_splits = 4\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "# Define hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'max_depth': [4, 6, 8, 12],\n",
    "    'random_state': [0],\n",
    "    'n_jobs': [-1],\n",
    "}\n",
    "\n",
    "splits = KFold(n_splits=n_splits, random_state=0, shuffle=True).split(train_dates)\n",
    "\n",
    "for i, (local_train_dates_ids, local_test_dates_ids) in enumerate(splits):\n",
    "    local_train_dates = train_dates[local_train_dates_ids]\n",
    "    local_test_dates = train_dates[local_test_dates_ids]\n",
    "\n",
    "    local_train_ids = train['DATE'].isin(local_train_dates)\n",
    "    local_test_ids = train['DATE'].isin(local_test_dates)\n",
    "\n",
    "    X_local_train = X_train.loc[local_train_ids]\n",
    "    y_local_train = y_train.loc[local_train_ids]\n",
    "    X_local_test = X_train.loc[local_test_ids]\n",
    "    y_local_test = y_train.loc[local_test_ids]\n",
    "\n",
    "    # Debugging NaN checks\n",
    "    assert not X_local_train.isnull().values.any(), \"X_local_train contains NaN values\"\n",
    "    assert not X_local_test.isnull().values.any(), \"X_local_test contains NaN values\"\n",
    "    assert not y_local_train.isnull().values.any(), \"y_local_train contains NaN values\"\n",
    "    assert not y_local_test.isnull().values.any(), \"y_local_test contains NaN values\"\n",
    "\n",
    "    # We implement some grid Search to find the best hyperparameters\n",
    "    rf = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "    grid_search.fit(X_local_train, y_local_train)\n",
    "\n",
    "    # We train the Random Forest with best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    model = RandomForestClassifier(**best_params)\n",
    "    model.fit(X_local_train, y_local_train)\n",
    "\n",
    "    y_local_pred = model.predict_proba(X_local_test)[:, 1]\n",
    "\n",
    "    sub = train.loc[local_test_ids].copy()\n",
    "    sub['pred'] = y_local_pred\n",
    "\n",
    "    # We adjust predictions to top 50% of returns for each date\n",
    "    y_local_pred = sub.groupby('DATE')['pred'].transform(lambda x: x > x.median()).values\n",
    "\n",
    "    # We append the results of the model to display their mean and standard deviation once the loop on the Kfolds is over.\n",
    "    models.append(model)\n",
    "    score = accuracy_score(y_local_test, y_local_pred)\n",
    "    scores.append(score)\n",
    "    print(f\"Fold {i+1} - Accuracy: {score* 100:.2f}%\")\n",
    "\n",
    "# Let's calculate the overall statistics to see if the model has improved.\n",
    "mean = np.mean(scores) * 100\n",
    "std = np.std(scores) * 100\n",
    "u = (mean + std)\n",
    "l = (mean - std)\n",
    "print(f'Accuracy: {mean:.2f}% [{l:.2f} ; {u:.2f}] (+- {std:.2f})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juPwzi7O9Th9"
   },
   "outputs": [],
   "source": [
    "# We can visualize which feature is the most important in the model we just built and see if the importance of features was changed with the previous and simpler model.\n",
    "feature_importances = pd.DataFrame([model.feature_importances_ for model in models], columns=features)\n",
    "\n",
    "sns.barplot(data=feature_importances, orient='h', order=feature_importances.mean().sort_values(ascending=False).index)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
